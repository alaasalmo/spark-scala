1. hadoop fs -put flights.csv  /demo/data/flights/
2. spark-shell --conf spark.ui.port=4545
3. --Prepare sqlContext
   import org.apache.spark.sql._
   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   import sqlContext.implicits._

4. -- Load
   val rdd=sc.textFile("/demo/data/flights/flights.csv")
   
5. take out header
   (I.) First    
   val header = rdd.first
   val dataRdd=rdd.filter(line=> line!=header)
   (II.) Second
   val dataRdd=rdd.zipWithIndex.filter{case (x,i)=> i!=0}.map{case (x,i)=>x}

6. parsing:
   val flightRdd=dataRdd.map(x=>x.split("\\,"))

7. create dataFrame

   case class Flight(
	Month: Int,
	DayofMonth: Int,
	DayOfWeek: Int,
	DepTime: Int,
	ArrTime: Int,
	UniqueCarrier: String,
	FlightNum: Int,
	TailNum: String,
	ActualElapsedTime: Int,
	AirTime: Int,
	ArrDelay: Int,
	DepDelay: Int,
	Origin: String,
	Dest: String,
	Distance: Int,
	TaxiIn: Int,
	TaxiOut: Int,
	Cancelled: String,
	CancellationCode: String,
	Diverted: String) 

   val flightDF=flightRdd.map(l=>Flight(l(0).toInt,l(1).toInt,l(2).toInt,l(3).toInt,l(4).toInt,l(5),l(6).toInt,l(7),l(8).toInt,l(9).toInt,l(10).toInt,l(11).toInt,l(12),l(13),l(14).toInt,l(15).toInt,l(16).toInt,l(17),l(18),l(19))).toDF
  
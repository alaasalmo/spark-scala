1. hadoop fs -put flights.csv  /demo/data/flights/
2. spark-shell --conf spark.ui.port=4545
3. --Prepare sqlContext
   import org.apache.spark.sql._
   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   import sqlContext.implicits._

4. -- Load
   val rdd=sc.textFile("/demo/data/flights/flights.csv")
   
5. take out header
   (I.) First    
   val header = rdd.first
   val dataRdd=rdd.filter(line=> line!=header)
   (II.) Second
   val dataRdd=rdd.zipWithIndex.filter{case (x,i)=> i!=0}.map{case (x,i)=>x}

6. parsing:
   val flightRdd=dataRdd.map(x=>x.split("\\,"))

7. create case class

   case class Flight(
	Month: Int,
	DayofMonth: Int,
	DayOfWeek: Int,
	DepTime: Int,
	ArrTime: Int,
	UniqueCarrier: String,
	FlightNum: Int,
	TailNum: String,
	ActualElapsedTime: Int,
	AirTime: Int,
	ArrDelay: Int,
	DepDelay: Int,
	Origin: String,
	Dest: String,
	Distance: Int,
	TaxiIn: Int,
	TaxiOut: Int,
	Cancelled: String,
	CancellationCode: String,
	Diverted: String) 

   
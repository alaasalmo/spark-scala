1. hadoop fs -put flights.csv  /demo/data/flights/
2. spark-shell --conf spark.ui.port=4545
3. --Prepare sqlContext
   import org.apache.spark.sql._
   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   import sqlContext.implicits._

4. -- Load
   val rdd=sc.textFile("/demo/data/flights/flights.csv")
   
5. take out header
   (I.) First    
   val header = rdd.first
   val dataRdd=rdd.filter(line=> line!=header)
   (II.) Second
   val dataRdd=rdd.zipWithIndex.filter{case (x,i)=> i!=0}.map{case (x,i)=>x}

6. parsing:
   val flightRdd=dataRdd.map(x=>x.split("\\,"))

7. create dataFrame

   case class Flight(
	Month: Int,
	DayofMonth: Int,
	DayOfWeek: Int,
	DepTime: Int,
	ArrTime: Int,
	UniqueCarrier: String,
	FlightNum: Int,
	TailNum: String,
	ActualElapsedTime: Int,
	AirTime: Int,
	ArrDelay: Int,
	DepDelay: Int,
	Origin: String,
	Dest: String,
	Distance: Int,
	TaxiIn: Int,
	TaxiOut: Int,
	Cancelled: String,
	CancellationCode: String,
	Diverted: String) 

   val flightDF=flightRdd.map(l=>Flight(l(0).toInt,l(1).toInt,l(2).toInt,l(3).toInt,l(4).toInt,l(5),l(6).toInt,l(7),l(8).toInt,l(9).toInt,l(10).toInt,l(11).toInt,l(12),l(13),l(14).toInt,l(15).toInt,l(16).toInt,l(17),l(18),l(19))).toDF

8. Using command to create DataFrame (instead of step 7)
   
   import org.apache.spark.sql.types.{StructType,StructField,StringType,IntegerType}
   import org.apache.spark.sql.Row
   
   val flightRW=flightRdd.map(l=>Row(l(0).toInt,l(1).toInt,l(2).toInt,l(3).toInt,l(4).toInt,l(5),l(6).toInt,l(7),l(8).toInt,l(9).toInt,l(10).toInt,l(11).toInt,l(12),l(13),l(14).toInt,l(15).toInt,l(16).toInt,l(17),l(18),l(19)))
   val flightSchema = StructType(Array(StructField("Month", IntegerType, false),StructField("DayofMonth", IntegerType, false),StructField("DayOfWeek", IntegerType, false),StructField("DepTime", IntegerType, false),StructField("ArrTime", IntegerType, false),StructField("UniqueCarrier", StringType, false),StructField("FlightNum", IntegerType, false),StructField("TailNum", StringType, false),StructField("ActualElapsedTime", IntegerType, false),StructField("AirTime", IntegerType, false),StructField("ArrDelay", IntegerType, false),StructField("DepDelay", IntegerType, false),StructField("Origin", StringType, false),StructField("Dest", StringType, false),StructField("Distance", IntegerType, false),StructField("TaxiIn", IntegerType, false),StructField("TaxiOut", IntegerType, false),StructField("Cancelled", StringType, false),StructField("CancellationCode", StringType, false),StructField("Diverted", StringType, false)))
   val peopleDataFrame = sqlContext.createDataFrame(flightRW, flightSchema)

9. Save the dataframe as a "parquet" file to the HDFS
   peopleDataFrame.write.format("parquet").save("/demo/data/flights/parquet")

10. val dfflight = sqlContext.read.format("parquet").load("/demo/data/flights/parquet")

11. Find the highest average delays by airport origin: dfflight.select($"Origin", $"DepDelay").//Try to finish

    dfflight.select($"Origin", $"DepDelay").groupBy($"Origin").avg("DepDelay").sort(desc("avg(DepDelay)")).show(5)
	dfflight.select($"Origin", $"DepDelay").groupBy($"Origin").avg("DepDelay").withColumnRenamed("avg(DepDelay)","DelayAvg").sort(desc("DelayAvg")).show(5)
	
12. Create a UDF to check if the flight is delayed or not:
    
	I.
	
    def delay_check(x:Int) : Int = {
        if(x>0) return 1
        else return 0
    }

    val depUDF = udf((x: Int) => delay_check(x))
	
    II. Select the columns using the UDF to check if a flight was delayed or not:

	 val delayDF = dfflight.select($"UniqueCarrier",depUDF($"DepDelay").alias("IsDelay"),$"DepDelay")

    III. Using groupBy, and the agg operator, create a count of the DepDelay to get total number of flights, and a sum of the IsDelayed Column:

    delayDF.groupBy("UniqueCarrier").agg(sum("DepDelay"),count("IsDelay")).show

    delayDF.groupBy("UniqueCarrier").agg("DepDelay" -> "sum","IsDelay" -> "count").show

    delayDF.groupBy("UniqueCarrier").agg(sum("DepDelay").alias("dep"),count("IsDelay").alias("isd")).sort(desc("dep"))show
    
    val delayGroupDF=delayDF.groupBy("UniqueCarrier").agg(sum("DepDelay"),count("IsDelay"))
    
    IV. Create a UDF to get the percentage of delayed flights:

    val calc_percent = udf((s: Int, c: Int) => s.toFloat/c)
    
    V. Create the final DataFrame by using a select, the UDF, and a sort, then show it:

    delayGroupDF.select($"UniqueCarrier",calc_percent($"sum(DepDelay)",$"count(IsDelay)").alias("Percentage")).sort("Percentage").show()
    
    c.  CHALLENGE: Find the top 5 airlines with longest average flight distance.
    
    
    
    delayDF.groupBy("UniqueCarrier").agg("DepDelay" -> "sum","IsDelay" -> "count").show

    delayDF.groupBy("UniqueCarrier").agg(sum("DepDelay").alias("dep"),count("IsDelay").alias("isd")).sort(desc("dep"))show
    
    val delayGroupDF=delayDF.groupBy("UniqueCarrier").agg(sum("DepDelay"),count("IsDelay"))
    
    IV. Create a UDF to get the percentage of delayed flights:

    val calc_percent = udf((s: Int, c: Int) => s.toFloat/c)
    
    V. Create the final DataFrame by using a select, the UDF, and a sort, then show it:

    delayGroupDF.select($"UniqueCarrier",calc_percent($"sum(DepDelay)",$"count(IsDelay)").alias("Percentage")).sort("Percentage").show()
    
    c.  CHALLENGE: Find the top 5 airlines with longest average flight distance.
    
    dfflight.groupBy("UniqueCarrier").avg("Distance").withColumnRenamed("avg(Distance)","DisAvg").sort(desc("DisAvg")).show(5)
    
    or
    
    dfflight.select($"UniqueCarrier",$"Distance").groupBy($"UniqueCarrier").avg().sort($"AVG(Distance)".desc).show(5)
    
